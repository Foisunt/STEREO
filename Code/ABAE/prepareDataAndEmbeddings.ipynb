{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import operator\n",
    "import codecs\n",
    "\n",
    "from tqdm import tqdm\n",
    "from argParseDummy import args\n",
    "from pathlib import Path\n",
    "\n",
    "args()\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cord19 Data (cord)\n",
    "\n",
    "#preprocess; takes ~15h for cord19 with 108k docs\n",
    "\n",
    "from preprocess import preprocess_Dataset\n",
    "preprocess_Dataset(args,0,-1)\n",
    "\n",
    "\n",
    "#vocab creation; takes 2 min for vocab processing and then 6 min per epoch\n",
    "\n",
    "from reader import create_vocab\n",
    "vocab = create_vocab(args.pre_path,args.cordVocab_path,args) \n",
    "vocabFile = open(args.cordVocabDict_path, \"w\") \n",
    "json.dump(vocab, vocabFile)\n",
    "vocabFile.close()\n",
    "\n",
    "\n",
    "#w2v\n",
    "\n",
    "from trainW2V import trainW2V\n",
    "trainW2V(args, source_path = args.pre_path, save_path = str(args.cordEmb_path), min_count=args.cordMin_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extracted (supp-sen)\n",
    "\n",
    "#preprocessing\n",
    "\n",
    "from preprocess import processExtracted\n",
    "sentSet, statCounter = processExtracted()\n",
    "print(\"stat counters:\", statCounter)\n",
    "print(\"sum of stat counters:\", sum(list(statCounter.values())))\n",
    "print(\"num sentences after stat and dupe filter:\", len(sentSet))\n",
    "\n",
    "#vocab\n",
    "\n",
    "from reader import create_vocab\n",
    "vocab = create_vocab(args.extractedProcessed_path,args.extrVocab_path,args) \n",
    "vocabFile = open(args.extrVocabDict_path, \"w\") \n",
    "json.dump(vocab, vocabFile)\n",
    "vocabFile.close()\n",
    "\n",
    "#w2v\n",
    "from trainW2V import trainW2V\n",
    "trainW2V(args, source_path = args.extractedProcessed_path, save_path = str(args.extrEmb_path), min_count = args.extrMin_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extracted + other (all-sen)\n",
    "\n",
    "#preprocess\n",
    "\n",
    "from preprocess import processExtOth\n",
    "sentSet = processExtOth()\n",
    "print(\"num sentences after dupe filter:\", len(sentSet))\n",
    "\n",
    "#vocab\n",
    "\n",
    "from reader import create_vocab\n",
    "vocab = create_vocab(args.extOthProcessed_path,args.extOthVocab_path,args) \n",
    "vocabFile = open(args.extOthVocabDict_path, \"w\") \n",
    "json.dump(vocab, vocabFile)\n",
    "vocabFile.close()\n",
    "\n",
    "#w2v\n",
    "\n",
    "from trainW2V import trainW2V\n",
    "trainW2V(args, source_path = args.extOthProcessed_path, save_path = str(args.extOthEmb_path), min_count = args.extOthMin_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
